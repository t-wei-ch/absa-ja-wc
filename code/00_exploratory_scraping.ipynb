{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 00. Exploratory Scraping\n",
    "\n",
    "As there is quite a volume of content that we would need to sift through in order to obtain the information that we want, and we ideally wish to do it in a minimal number of attempts.\n",
    "\n",
    "Thus, the purpose of this notebook is purely to explore the structure of the [Rakuten Travel](https://travel.rakuten.co.jp/) webpage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the Homepage\n",
    "\n",
    "We first scrape the [homepage](https://travel.rakuten.co.jp/) of Rakuten Travel to see how we can access the reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "homepage_res = requests.get(url='https://travel.rakuten.co.jp/')\n",
    "homepage_soup = BeautifulSoup(homepage_res.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(homepage_soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we are getting some encoding issues here. Let's compare the apparent and actual encodings of the scraped website text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(homepage_res.encoding)\n",
    "print(homepage_res.apparent_encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As suspected, there is a discrepancy in the encoding used. Let's force the encoding to be UTF-8 (what it is in the content) so that we can actually read the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "homepage_res.encoding = 'utf-8'\n",
    "homepage_soup = BeautifulSoup(homepage_res.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(homepage_soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that seems to have fixed the problem. \n",
    "\n",
    "Now we can proceed to get the names of each individual location, which are located in `<dd class=\"area dmArea\">` tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "areas = homepage_soup.find('dd', attrs={'class': 'area dmArea'})\n",
    "areas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tells us the names of the different prefectures, but not really enough to navigate to the correct link.\n",
    "\n",
    "Let us take a closer look at how links between two prefectures differ. For the sake of this comparison, we shall compare the Tokyo and Aomori prefectures. Both links start with `https://search.travel.rakuten.co.jp/ds/undated/search?`, but where they differ is in their optional parameters.\n",
    "\n",
    "|Prefecture|URL|\n",
    "|---|---|\n",
    "|Tokyo|`https://search.travel.rakuten.co.jp/ds/undated/search?f_dai=japan&f_sort=hotel&f_page=1&f_hyoji=30&f_tab=hotel&f_cd=02&`<br>`f_layout=list&f_campaign=&` `f_chu=tokyo` `&f_shou=&f_sai=&f_charge_users=&l-id=topC_search_hotel_undated`|\n",
    "|Aomori|`https://search.travel.rakuten.co.jp/ds/undated/search?f_dai=japan&f_sort=hotel&f_page=1&f_hyoji=30&f_tab=hotel&f_cd=02&`<br>`f_layout=list&f_campaign=&` `f_chu=aomori` `&f_shou=&f_sai=&f_charge_users=&l-id=topC_search_hotel_undated`|\n",
    "\n",
    "As we can see, the only place where the two urls differ is in the `f_chu` parameter, which takes in the name of the prefecture present in the `<option value=___>` tag on the homepage.\n",
    "\n",
    "This allows us to programmatically search through all the prefectures and get all their hotels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us get a list of all the prefecture names that goes into the `f_chu` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_prefectures = [pref_tag.get('value') for pref_tag in areas.findAll('option')]\n",
    "len(list_of_prefectures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_prefectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sanity check:** There are only 47 prefectures in Japan, but we seem to be getting repeats.\n",
    "\n",
    "Looking at the `list_of_prefectures` variable above, we see that two prefectures (Kanagawa and Shizuoka) are repeated (with different `id`s, as seen in the `area` variable).\n",
    "\n",
    "However, as the `value` for both versions of the two prefectures are identical, it is safe to simply collapse those together.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_prefectures = list(set(list_of_prefectures))\n",
    "len(list_of_prefectures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_prefectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we get 47 prefectures, as intended.\n",
    "\n",
    "As an aside, while most English speakers would know prefectures such as 福島 and 千葉 by their [Hepburn romanization](https://en.wikipedia.org/wiki/Hepburn_romanization) - Fukushima and Chiba, respectively, this page makes use of the [Nihon-shiki romanization](https://en.wikipedia.org/wiki/Nihon-shiki_romanization) (Japanese-style romanization) used more commonly in Japan, which is written as Hukushima and Tiba respectively. Even then, it is slightly different, as 福島 written in proper Nihon-shiki romanization would be \"hukusima\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can follow the same procedure in navigating all the way to the last page - checking the `<li class=\"pagingBack\">` tag."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reviews from Individual Hotels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prefecture_url_tokyo = 'https://search.travel.rakuten.co.jp/ds/undated/search?f_dai=japan&f_sort=hotel&f_page=1&f_hyoji=30&f_tab=hotel&f_cd=02&f_layout=list&f_campaign=&f_chu=tokyo&f_shou=&f_sai=&f_charge_users=&l-id=topC_search_hotel_undated'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokyo_res = requests.get(test_prefecture_url_tokyo)\n",
    "tokyo_soup = BeautifulSoup(tokyo_res.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokyo_soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The webpage we are currently in lists out all the hotels in that prefecture by page. While we could in theory go into the actual hotel page and access the reviews from there, the link already exists in the same box as the hotel page, beside the hotel rating.\n",
    "\n",
    "It can be directly accessed by searching for the `<p class=\"cstmrEvl\">` tag. As usual, the links are stored inside `<a href=___>` tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokyo_review_links = [hotel.find('a').get('href') for hotel in tokyo_soup.findAll('p', attrs={'class': 'cstmrEvl'})]\n",
    "tokyo_review_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping a single review\n",
    "\n",
    "We start off by taking small steps with scraping a single page of reviews, then slowly scope up as we go along."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = requests.get(url='https://travel.rakuten.co.jp/HOTEL/28096/review.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(res.text)\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see immediately that the Japanese text isn't being rendered properly. This is an encoding problem, and is easily remedied by declaring the encoding of the response from `requests` to be UTF-8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this forces the encoding to be UTF-8\n",
    "# otherwise the output would be unreadable\n",
    "res.encoding = 'utf-8'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(res.text)\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see from a cursory inspection that reviews are in the `<p class=\"commentSentence\">` tags. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = [review.text for review in soup.findAll('p', attrs={'class': 'commentSentence'})]\n",
    "len(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, they seem to have some extra formatting noise, mainly newline characters. Let us remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = [re.sub('\\\\[rn]', '', review).strip() for review in reviews]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better. Now, we also notice that half of the \"reviews\" are in fact replies by the hotel. As those replies always start with the same line(s) of greetings and thanks. This makes it a lot easier for us to filter out such comments.\n",
    "\n",
    "Later on, we shall encounter a more efficient way to gather specifically guest reviews or hotel replies, but this is exploratory, so let's flow with this for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guest_reviews = [review for review in reviews if 'この度はホテルマイステイズ浅草' not in review]\n",
    "len(guest_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "guest_reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for the more efficient way of determining which commments are by actual guests, and which are from hotel replies, going up one tag level makes it clearer that guest reviews and hotel replies are wrapped under different `<dl>` tags.\n",
    "\n",
    "The tag `<div class=\"commentReputationBoth\">` captures both the user review and the reply by the hotel front desk.\n",
    "\n",
    "The user reviews are under the tag `<dl class=\"commentReputation\">`, and the hotel replies are under the tag `<dl class=\"commentHotel\">`.\n",
    "\n",
    "If one were to only look at comments without any care for who they come from, simply searching for tags with `<p class=\"commentSentence\">` would suffice.\n",
    "\n",
    "Additionally, each review has a specific id, which can be found in the first `<div class=\"voteQuestion\">` tag, and is accessed through the `id` tag. This would return a string `voteans_[id number]`, which we shall use.\n",
    "\n",
    "As we are mainly concerned with user reviews, we shall first extract only those under the `<dl class=\"commentReputation\">` tag.\n",
    "\n",
    "If we ever decide to work with hotel replies, we can simply just extract them under the `<dl class=\"commentHotel\">` tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup_comments_meta = soup.findAll('div', attrs={'class': 'commentBox'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(soup_comments_meta[0].prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking Forward - Aspect-Based Sentiment Analysis\n",
    "\n",
    "We need a way to obtain aspects in order to perform aspect-based sentiment analysis.\n",
    "\n",
    "Thankfully, Rakuten Travel already splits up their guest ratings into six categories:\n",
    "- Service (サービス)\n",
    "- Location (立地)\n",
    "- Room (部屋)\n",
    "- Amenities (設備・アメニティ)\n",
    "- Bathroom (風呂)\n",
    "- Meals (食事)\n",
    "\n",
    "Each category is rated on a scale of 1 to 5, and gives us an idea of what the guest liked or did not like about the experience.\n",
    "\n",
    "These six scores are then aggregated to give a total score (総合) on a scale of 1 to 5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we have to get the url that links to the scores. \n",
    "\n",
    "Every customer review contains a link to a more detailed review page with their score breakdown for the six categories. This would be the very first `href` tag that appears under each `<div class=\"commentBox\">` tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_breakdown_link = soup_comments_meta[0].find('a').get('href')\n",
    "review_breakdown_link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now scrape the link to the individual's review details, and a cursory scan reveals that the score data is located under the `<ul class=\"rateDetail\">` tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_review_res = requests.get(url=review_breakdown_link)\n",
    "test_review_soup = BeautifulSoup(test_review_res.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_review_soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There should only be one element that has the tag `<ul class=\"rateDetail\">` in the reviews page.\n",
    "\n",
    "Extracting the text, and then splitting on the newline `\\n` character, would then give us the necessary review scores for a single customer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_string = test_review_soup.find('ul', attrs={'class': 'rateDetail'}).text\n",
    "scores_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_scores = scores_string.split('\\n')[1:-1] # the string starts and ends with a newline character\n",
    "test_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next page can be accessed by the link with the text `次の__件`. This is encapsulated in the first `href` in the `<li class=\"pagingNext\">` tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# next page\n",
    "next_page = soup.find('li', attrs={'class': 'pagingNext'}).find('a').get('href')\n",
    "next_page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check what happens when we reach the end of the reviews. As it so happens, at the time of writing, the current hotel page only has two pages of reviews. This makes it quite an illustrating minimal example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_page_res = requests.get(url=next_page)\n",
    "next_page_soup = BeautifulSoup(next_page_res.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(next_page_soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check if there's a link to the next page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next_page_soup.find('li', attrs={'class': 'pagingNext'}) == None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This serves as a nice check for when we reach the end of the comments, so that we know when to call it a day with a specific hotel and move on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know how to deal with a single hotel's reviews, let's look at how we can loop through all the hotels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall Scraping Strategy\n",
    "\n",
    "Let us now put everything we have uncovered above together to obtain the data that we need.\n",
    "\n",
    "We shall do this by writing out some pseudocode before actually attempting the scrape.\n",
    "\n",
    "---\n",
    "\n",
    "Initialize empty lists in a dictionary for the columns that we want:<br>\n",
    "`review_id`, `review_time`, `review_text`, `hotel_reply_time`, `hotel_reply_text`, and the 7 scores in the order of overall, service, location, room, amenities, bathroom, and food.\n",
    "\n",
    "Scrape the Rakuten homepage and create the homepage soup.\n",
    "\n",
    "Encode the response to utf-8 first.\n",
    "\n",
    "From the homepage soup, extract out the list of prefecture names.\n",
    "\n",
    "**for loop 1: From homepage, looping through prefectures** <br>\n",
    "- `for prefecture in prefecture_names:`\n",
    "    - scrape link to prefecture hotels list and make soup\n",
    "\n",
    "    - initialize an empty list for the hotel links\n",
    "    - extract list of review links (`findAll('p', attrs={'class': 'cstmrEvl'})` and `find('a').get('href')`)\n",
    "    - `while prefecture_hotels_soup.find('li', attrs={'class': 'pagingNext'}) != None:`\n",
    "        - go to next page\n",
    "    \n",
    "    - **for loop 2: From prefecture, looping through hotels** \n",
    "    - `for hotel_review_link in list_of_hotel_review_links:`\n",
    "        - make soup\n",
    "\n",
    "        - initialize an empty list for the customers\n",
    "        - extract list of customer review details links (`findAll('div', attrs={'class': 'commentBox'})` and `find('a').get('href')`)\n",
    "        - `while hotel_soup.find('li', attrs={'class': 'pagingNext'}) != None:`\n",
    "            - go to next page\n",
    "\n",
    "        - **for loop 3: From hotel, looping through customers**\n",
    "        - `for customer_review on list_of_customer_reviews:`\n",
    "            - make soup\n",
    "            - encode the response to utf-8 first\n",
    "\n",
    "            - extract review id (`find('div', class='voteQuestion').get('id')`)\n",
    "\n",
    "            - extract review timestamp (`find('span', attrs={'class': 'time'}).text`)\n",
    "\n",
    "            - extract review text (`find('p', attrs={'class': 'commentSentence'})`)\n",
    "\n",
    "            - try\n",
    "                - make hotel reply soup (if it exists)\n",
    "                - extract hotel reply timestamp\n",
    "                - extract hotel reply text\n",
    "            \n",
    "            - extract hotel name (`find('a', attrs={'class': 'rtconds fn'}).text`)\n",
    "\n",
    "            - extract scores as a string (`find('ul', attrs={'class': 'rateDetail'}).text`)\n",
    "\n",
    "            - split scores (`split('\\n')[1:-1]`)\n",
    "\n",
    "            - append review id\n",
    "            - append review time\n",
    "            - append review text\n",
    "            - append hotel reply timestamp\n",
    "            - append hotel reply text\n",
    "            - append hotel name\n",
    "            - append prefecture\n",
    "            - `for i in range(7):`\n",
    "                - we only want the number, the order of the categories is already encoded in the initialization above\n",
    "                - `lists_of_scores[i].append(customer_scores[i][-1])`\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
