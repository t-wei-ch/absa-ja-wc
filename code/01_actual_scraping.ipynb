{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import requests\n",
    "import time\n",
    "\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overall Scraping Strategy\n",
    "\n",
    "As a convenient reference for writing the actual scraping code, the pseudocode at the end of the previous notebook is copied here.\n",
    "\n",
    "---\n",
    "\n",
    "Initialize empty lists in a dictionary for the columns that we want:<br>\n",
    "`review_id`, `review_time`, `review_text`, `hotel_reply_time`, `hotel_reply_text`, and the 7 scores in the order of overall, service, location, room, amenities, bathroom, and food.\n",
    "\n",
    "Scrape the Rakuten homepage and create the homepage soup.\n",
    "\n",
    "Encode the response to utf-8 first.\n",
    "\n",
    "From the homepage soup, extract out the list of prefecture names.\n",
    "\n",
    "**for loop 1: From homepage, looping through prefectures** <br>\n",
    "- `for prefecture in prefecture_names:`\n",
    "    - scrape link to prefecture hotels list and make soup\n",
    "\n",
    "    - initialize an empty list for the hotel links\n",
    "    - extract list of review links (`findAll('p', attrs={'class': 'cstmrEvl'})` and `find('a').get('href')`)\n",
    "    - `while prefecture_hotels_soup.find('li', attrs={'class': 'pagingNext'}) != None:`\n",
    "        - go to next page\n",
    "    \n",
    "    - **for loop 2: From prefecture, looping through hotels** \n",
    "    - `for hotel_review_link in list_of_hotel_review_links:`\n",
    "        - make soup\n",
    "\n",
    "        - initialize an empty list for the customers\n",
    "        - extract list of customer review details links (`findAll('div', attrs={'class': 'commentBox'})` and `find('a').get('href')`)\n",
    "        - `while hotel_soup.find('li', attrs={'class': 'pagingNext'}) != None:`\n",
    "            - go to next page\n",
    "\n",
    "        - **for loop 3: From hotel, looping through customers**\n",
    "        - `for customer_review on list_of_customer_reviews:`\n",
    "            - make soup\n",
    "            - encode the response to utf-8 first\n",
    "\n",
    "            - extract review id (`find('div', class='voteQuestion').get('id')`)\n",
    "\n",
    "            - extract review timestamp (`find('span', attrs={'class': 'time'}).text`)\n",
    "\n",
    "            - extract review text (`find('p', attrs={'class': 'commentSentence'})`)\n",
    "\n",
    "            - try\n",
    "                - make hotel reply soup (if it exists)\n",
    "                - extract hotel reply timestamp\n",
    "                - extract hotel reply text\n",
    "            \n",
    "            - extract hotel name (`find('a', attrs={'class': 'rtconds fn'}).text`)\n",
    "\n",
    "            - extract scores as a string (`find('ul', attrs={'class': 'rateDetail'}).text`)\n",
    "\n",
    "            - split scores (`split('\\n')[1:-1]`)\n",
    "\n",
    "            - append review id\n",
    "            - append review time\n",
    "            - append review text\n",
    "            - append hotel reply timestamp\n",
    "            - append hotel reply text\n",
    "            - append hotel name\n",
    "            - append prefecture\n",
    "            - `for i in range(7):`\n",
    "                - we only want the number, the order of the categories is already encoded in the initialization above\n",
    "                - `lists_of_scores[i].append(customer_scores[i][-1])`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_rakuten():\n",
    "    '''This is a script to scrape Rakuten Travel, as outlined in the accompanying pseudocode.\n",
    "\n",
    "    Returns\n",
    "    ---\n",
    "    A dictionary, where each component list is a column of the DataFrame we want to end up with.\n",
    "\n",
    "    This will later be exported to a `.csv` file.\n",
    "    '''\n",
    "\n",
    "    ########## output lists ##########\n",
    "    final_dict = {'review_id': [],\n",
    "                  'review_time': [],\n",
    "                  'review_text': [],\n",
    "                  'hotel_reply_time': [],\n",
    "                  'hotel_reply_text': [],\n",
    "                  'hotel_name': [],\n",
    "                  'prefecture': [],\n",
    "                  'overall_score': [],\n",
    "                  'service_score': [],\n",
    "                  'location_score': [],\n",
    "                  'room_score': [],\n",
    "                  'amenities_score': [],\n",
    "                  'bathroom_score': [],\n",
    "                  'food_score': []}\n",
    "    \n",
    "    score_order = ['overall_score', \n",
    "                   'service_score', \n",
    "                   'location_score',\n",
    "                   'room_score',\n",
    "                   'amenities_score',\n",
    "                   'bathroom_score',\n",
    "                   'food_score']\n",
    "    ##################################\n",
    "\n",
    "\n",
    "\n",
    "    # initial scraping of Rakuten Travel homepage\n",
    "    # we only need to do this once\n",
    "    homepage_res = requests.get(url='https://travel.rakuten.co.jp/')\n",
    "    homepage_res.encoding = 'utf-8'\n",
    "    homepage_soup = BeautifulSoup(homepage_res.text)\n",
    "\n",
    "    # extracts out the list of 47 unique prefectures\n",
    "    areas = homepage_soup.find('dd', attrs={'class': 'area dmArea'})\n",
    "    list_of_prefectures = [pref_tag.get('value') for pref_tag in areas.findAll('option')]\n",
    "    list_of_prefectures = list(set(list_of_prefectures))\n",
    "\n",
    "    # due to the time constraint, we limit the scraping to hotels from the Tohoku region\n",
    "    # Aomori, Iwate, Miyagi, Akita, Yamagata, Fukushima\n",
    "    list_of_prefectures = ['aomori', 'iwate', 'miyagi', 'akita', 'yamagata', 'hukushima']\n",
    "\n",
    "    # loop 1: homepage -> prefectures\n",
    "    for prefecture in list_of_prefectures:\n",
    "        # sets the correct url\n",
    "        pref_url = f\"https://search.travel.rakuten.co.jp/ds/undated/search?f_dai=japan&f_sort=hotel&f_page=1&f_hyoji=30&f_tab=hotel&f_cd=02&f_layout=list&f_campaign=&f_chu={prefecture}&f_shou=&f_sai=&f_charge_users=&l-id=topC_search_hotel_undated\"\n",
    "        pref_res = requests.get(url=pref_url)\n",
    "        pref_res.encoding = 'utf-8'\n",
    "        pref_soup = BeautifulSoup(pref_res.text)\n",
    "\n",
    "        # initializes list of individual hotel review links\n",
    "        list_of_hotel_review_links = []\n",
    "\n",
    "        # this gets all the hotels from the first page\n",
    "        for _ in range(1):\n",
    "            # appends hotel review links\n",
    "            list_of_hotel_review_links.extend([hotel.find('a').get('href') for hotel \n",
    "                                               in pref_soup.findAll('p', attrs={'class': 'cstmrEvl'}) \n",
    "                                               if hotel.find('a') != None])\n",
    "\n",
    "            # ends the while loop if there is no next page\n",
    "            if pref_soup.find('li', attrs={'class': 'pagingNext'}) == None:\n",
    "                break\n",
    "            else:\n",
    "                # gets the next page's url\n",
    "                next_hotel_page_url = pref_soup.find('li', attrs={'class': 'pagingNext'}).find('a').get('href')\n",
    "                next_hotel_page_res = requests.get(url=next_hotel_page_url)\n",
    "                next_hotel_page_res.encoding = 'utf-8'\n",
    "                # overwrites pref_soup so that the while loop terminates properly\n",
    "                pref_soup = BeautifulSoup(next_hotel_page_res.text)\n",
    "        \n",
    "        # this is to keep track of progress\n",
    "        counter = 0\n",
    "\n",
    "        # loop 2: prefectures -> hotels\n",
    "        for hotel_review_link in list_of_hotel_review_links:\n",
    "            # makes soup\n",
    "            hotel_res = requests.get(hotel_review_link)\n",
    "            hotel_res.encoding = 'utf-8'\n",
    "            hotel_soup = BeautifulSoup(hotel_res.text)\n",
    "\n",
    "            # initializes list of customer review links\n",
    "            list_of_customer_review_links = []\n",
    "\n",
    "            # this gets all reviews from the first 2 pages\n",
    "            for _ in range(2):\n",
    "                # appends each review's individual link\n",
    "                list_of_customer_review_links.extend([comment_tag.find('a').get('href') for comment_tag \n",
    "                                                      in hotel_soup.findAll('div', attrs={'class': 'commentBox'})])\n",
    "\n",
    "                # ends the loop if there is no next page\n",
    "                if hotel_soup.find('li', attrs={'class': 'pagingNext'}) == None:\n",
    "                    break\n",
    "                else:\n",
    "                    # gets the next page's url\n",
    "                    next_review_page_url = hotel_soup.find('li', attrs={'class': 'pagingNext'}).find('a').get('href')\n",
    "                    next_review_page_res = requests.get(url=next_review_page_url)\n",
    "                    next_review_page_res.encoding = 'utf-8'\n",
    "                    # overwrites hotel_soup so that the while loop terminates properly\n",
    "                    hotel_soup = BeautifulSoup(next_review_page_res.text)\n",
    "            \n",
    "            # loop 3: hotels -> customer reviews\n",
    "            for customer_review_link in list_of_customer_review_links:\n",
    "                # makes soup\n",
    "                review_res = requests.get(customer_review_link)\n",
    "                time.sleep(1) # throttling to prevent overloading Rakuten's servers\n",
    "                review_res.encoding = 'utf-8'\n",
    "                review_soup = BeautifulSoup(review_res.text)\n",
    "\n",
    "                # extracts the html for the customer review only\n",
    "                customer_review_soup = review_soup.find('dl', attrs={'class': 'commentReputation'})\n",
    "\n",
    "                # extracts review id\n",
    "                review_id = customer_review_soup.find('div', attrs={'class': 'voteQuestion'}).get('id')\n",
    "                \n",
    "                # extracts review timestamp\n",
    "                review_time = customer_review_soup.find('span', attrs={'class': 'time'}).text\n",
    "\n",
    "                # extracts review text\n",
    "                review_text = customer_review_soup.find('p', attrs={'class': 'commentSentence'}).text\n",
    "                # removes newline characters and trailing whitespaces\n",
    "                review_text = re.sub('\\\\[rn]', '', review_text).strip()\n",
    "\n",
    "                # extracts the html for hotel reply (if it exists)\n",
    "                try:\n",
    "                    hotel_reply_soup = review_soup.find('dl', attrs={'class': 'commentHotel'})\n",
    "\n",
    "                    # extracts timestamp of hotel's reply\n",
    "                    hotel_reply_time = hotel_reply_soup.find('span', attrs={'class': 'time'}).text\n",
    "\n",
    "                    # extracts hotel reply\n",
    "                    hotel_reply_text = hotel_reply_soup.find('p', attrs={'class': 'commentSentence'}).text\n",
    "                    hotel_reply_text = re.sub('\\\\[rn]', '', hotel_reply_text)\n",
    "                except AttributeError: # in the case that the hotel does not leave a reply\n",
    "                    hotel_reply_time = np.nan\n",
    "                    hotel_reply_text = np.nan\n",
    "\n",
    "                # extracts hotel name\n",
    "                hotel_name = review_soup.find('a', attrs={'class': 'rtconds fn'}).text\n",
    "\n",
    "                try: \n",
    "                    # extracts the respective scores\n",
    "                    review_scores_string = customer_review_soup.find('ul', attrs={'class': 'rateDetail'}).text\n",
    "                    # splits the scores into a list\n",
    "                    review_scores = review_scores_string.split('\\n')[1:-1]\n",
    "                except AttributeError: # sometimes people don't vote\n",
    "                    continue\n",
    "\n",
    "                # appending data to the respective list\n",
    "                final_dict['review_id'].append(review_id)\n",
    "                final_dict['review_time'].append(review_time)\n",
    "                final_dict['review_text'].append(review_text)\n",
    "                final_dict['hotel_reply_time'].append(hotel_reply_time)\n",
    "                final_dict['hotel_reply_text'].append(hotel_reply_text)\n",
    "                final_dict['hotel_name'].append(hotel_name)\n",
    "                final_dict['prefecture'].append(prefecture)\n",
    "                for i in range(7):\n",
    "                    final_dict[score_order[i]].append(review_scores[i][-1])\n",
    "                \n",
    "            # this is here just to indicate progress\n",
    "            counter += 1\n",
    "            print(f'Scraping completed for {prefecture.capitalize()} hotel {counter}.')\n",
    "\n",
    "        # this is here just to indicate progress\n",
    "        print(f'Scraping completed for {prefecture.capitalize()} prefecture.')\n",
    "\n",
    "    print('All hotels scraped.')\n",
    "\n",
    "    return final_dict\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first scrape the data into a dictionary `rakuten_dict`.\n",
    "\n",
    "Just the top 2 pages of reviews for the first page of hotels for just the Tohoku region (Aomori, Iwate, Miyagi, Akita, Yamagata, Fukushima prefectures) takes around 4 hours total to scrape. This totals to around 40 reviews each from 187 hotels.\n",
    "\n",
    "If time had permitted, and the better processors were had, scraping all the hotels in all 47 prefectures would be done. However, it would take days of nonstop scraping in order to get all the data, because the individual scores only show up in the title link of each individual review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping completed for Aomori hotel 1.\n",
      "Scraping completed for Aomori hotel 2.\n",
      "Scraping completed for Aomori hotel 3.\n",
      "Scraping completed for Aomori hotel 4.\n",
      "Scraping completed for Aomori hotel 5.\n",
      "Scraping completed for Aomori hotel 6.\n",
      "Scraping completed for Aomori hotel 7.\n",
      "Scraping completed for Aomori hotel 8.\n",
      "Scraping completed for Aomori hotel 9.\n",
      "Scraping completed for Aomori hotel 10.\n",
      "Scraping completed for Aomori hotel 11.\n",
      "Scraping completed for Aomori hotel 12.\n",
      "Scraping completed for Aomori hotel 13.\n",
      "Scraping completed for Aomori hotel 14.\n",
      "Scraping completed for Aomori hotel 15.\n",
      "Scraping completed for Aomori hotel 16.\n",
      "Scraping completed for Aomori hotel 17.\n",
      "Scraping completed for Aomori hotel 18.\n",
      "Scraping completed for Aomori hotel 19.\n",
      "Scraping completed for Aomori hotel 20.\n",
      "Scraping completed for Aomori hotel 21.\n",
      "Scraping completed for Aomori hotel 22.\n",
      "Scraping completed for Aomori hotel 23.\n",
      "Scraping completed for Aomori hotel 24.\n",
      "Scraping completed for Aomori hotel 25.\n",
      "Scraping completed for Aomori hotel 26.\n",
      "Scraping completed for Aomori hotel 27.\n",
      "Scraping completed for Aomori hotel 28.\n",
      "Scraping completed for Aomori hotel 29.\n",
      "Scraping completed for Aomori hotel 30.\n",
      "Scraping completed for Aomori hotel 31.\n",
      "Scraping completed for Aomori prefecture.\n",
      "Scraping completed for Iwate hotel 1.\n",
      "Scraping completed for Iwate hotel 2.\n",
      "Scraping completed for Iwate hotel 3.\n",
      "Scraping completed for Iwate hotel 4.\n",
      "Scraping completed for Iwate hotel 5.\n",
      "Scraping completed for Iwate hotel 6.\n",
      "Scraping completed for Iwate hotel 7.\n",
      "Scraping completed for Iwate hotel 8.\n",
      "Scraping completed for Iwate hotel 9.\n",
      "Scraping completed for Iwate hotel 10.\n",
      "Scraping completed for Iwate hotel 11.\n",
      "Scraping completed for Iwate hotel 12.\n",
      "Scraping completed for Iwate hotel 13.\n",
      "Scraping completed for Iwate hotel 14.\n",
      "Scraping completed for Iwate hotel 15.\n",
      "Scraping completed for Iwate hotel 16.\n",
      "Scraping completed for Iwate hotel 17.\n",
      "Scraping completed for Iwate hotel 18.\n",
      "Scraping completed for Iwate hotel 19.\n",
      "Scraping completed for Iwate hotel 20.\n",
      "Scraping completed for Iwate hotel 21.\n",
      "Scraping completed for Iwate hotel 22.\n",
      "Scraping completed for Iwate hotel 23.\n",
      "Scraping completed for Iwate hotel 24.\n",
      "Scraping completed for Iwate hotel 25.\n",
      "Scraping completed for Iwate hotel 26.\n",
      "Scraping completed for Iwate hotel 27.\n",
      "Scraping completed for Iwate hotel 28.\n",
      "Scraping completed for Iwate hotel 29.\n",
      "Scraping completed for Iwate hotel 30.\n",
      "Scraping completed for Iwate hotel 31.\n",
      "Scraping completed for Iwate hotel 32.\n",
      "Scraping completed for Iwate prefecture.\n",
      "Scraping completed for Miyagi hotel 1.\n",
      "Scraping completed for Miyagi hotel 2.\n",
      "Scraping completed for Miyagi hotel 3.\n",
      "Scraping completed for Miyagi hotel 4.\n",
      "Scraping completed for Miyagi hotel 5.\n",
      "Scraping completed for Miyagi hotel 6.\n",
      "Scraping completed for Miyagi hotel 7.\n",
      "Scraping completed for Miyagi hotel 8.\n",
      "Scraping completed for Miyagi hotel 9.\n",
      "Scraping completed for Miyagi hotel 10.\n",
      "Scraping completed for Miyagi hotel 11.\n",
      "Scraping completed for Miyagi hotel 12.\n",
      "Scraping completed for Miyagi hotel 13.\n",
      "Scraping completed for Miyagi hotel 14.\n",
      "Scraping completed for Miyagi hotel 15.\n",
      "Scraping completed for Miyagi hotel 16.\n",
      "Scraping completed for Miyagi hotel 17.\n",
      "Scraping completed for Miyagi hotel 18.\n",
      "Scraping completed for Miyagi hotel 19.\n",
      "Scraping completed for Miyagi hotel 20.\n",
      "Scraping completed for Miyagi hotel 21.\n",
      "Scraping completed for Miyagi hotel 22.\n",
      "Scraping completed for Miyagi hotel 23.\n",
      "Scraping completed for Miyagi hotel 24.\n",
      "Scraping completed for Miyagi hotel 25.\n",
      "Scraping completed for Miyagi hotel 26.\n",
      "Scraping completed for Miyagi hotel 27.\n",
      "Scraping completed for Miyagi hotel 28.\n",
      "Scraping completed for Miyagi hotel 29.\n",
      "Scraping completed for Miyagi hotel 30.\n",
      "Scraping completed for Miyagi hotel 31.\n",
      "Scraping completed for Miyagi prefecture.\n",
      "Scraping completed for Akita hotel 1.\n",
      "Scraping completed for Akita hotel 2.\n",
      "Scraping completed for Akita hotel 3.\n",
      "Scraping completed for Akita hotel 4.\n",
      "Scraping completed for Akita hotel 5.\n",
      "Scraping completed for Akita hotel 6.\n",
      "Scraping completed for Akita hotel 7.\n",
      "Scraping completed for Akita hotel 8.\n",
      "Scraping completed for Akita hotel 9.\n",
      "Scraping completed for Akita hotel 10.\n",
      "Scraping completed for Akita hotel 11.\n",
      "Scraping completed for Akita hotel 12.\n",
      "Scraping completed for Akita hotel 13.\n",
      "Scraping completed for Akita hotel 14.\n",
      "Scraping completed for Akita hotel 15.\n",
      "Scraping completed for Akita hotel 16.\n",
      "Scraping completed for Akita hotel 17.\n",
      "Scraping completed for Akita hotel 18.\n",
      "Scraping completed for Akita hotel 19.\n",
      "Scraping completed for Akita hotel 20.\n",
      "Scraping completed for Akita hotel 21.\n",
      "Scraping completed for Akita hotel 22.\n",
      "Scraping completed for Akita hotel 23.\n",
      "Scraping completed for Akita hotel 24.\n",
      "Scraping completed for Akita hotel 25.\n",
      "Scraping completed for Akita hotel 26.\n",
      "Scraping completed for Akita hotel 27.\n",
      "Scraping completed for Akita hotel 28.\n",
      "Scraping completed for Akita hotel 29.\n",
      "Scraping completed for Akita hotel 30.\n",
      "Scraping completed for Akita hotel 31.\n",
      "Scraping completed for Akita prefecture.\n",
      "Scraping completed for Yamagata hotel 1.\n",
      "Scraping completed for Yamagata hotel 2.\n",
      "Scraping completed for Yamagata hotel 3.\n",
      "Scraping completed for Yamagata hotel 4.\n",
      "Scraping completed for Yamagata hotel 5.\n",
      "Scraping completed for Yamagata hotel 6.\n",
      "Scraping completed for Yamagata hotel 7.\n",
      "Scraping completed for Yamagata hotel 8.\n",
      "Scraping completed for Yamagata hotel 9.\n",
      "Scraping completed for Yamagata hotel 10.\n",
      "Scraping completed for Yamagata hotel 11.\n",
      "Scraping completed for Yamagata hotel 12.\n",
      "Scraping completed for Yamagata hotel 13.\n",
      "Scraping completed for Yamagata hotel 14.\n",
      "Scraping completed for Yamagata hotel 15.\n",
      "Scraping completed for Yamagata hotel 16.\n",
      "Scraping completed for Yamagata hotel 17.\n",
      "Scraping completed for Yamagata hotel 18.\n",
      "Scraping completed for Yamagata hotel 19.\n",
      "Scraping completed for Yamagata hotel 20.\n",
      "Scraping completed for Yamagata hotel 21.\n",
      "Scraping completed for Yamagata hotel 22.\n",
      "Scraping completed for Yamagata hotel 23.\n",
      "Scraping completed for Yamagata hotel 24.\n",
      "Scraping completed for Yamagata hotel 25.\n",
      "Scraping completed for Yamagata hotel 26.\n",
      "Scraping completed for Yamagata hotel 27.\n",
      "Scraping completed for Yamagata hotel 28.\n",
      "Scraping completed for Yamagata hotel 29.\n",
      "Scraping completed for Yamagata hotel 30.\n",
      "Scraping completed for Yamagata prefecture.\n",
      "Scraping completed for Hukushima hotel 1.\n",
      "Scraping completed for Hukushima hotel 2.\n",
      "Scraping completed for Hukushima hotel 3.\n",
      "Scraping completed for Hukushima hotel 4.\n",
      "Scraping completed for Hukushima hotel 5.\n",
      "Scraping completed for Hukushima hotel 6.\n",
      "Scraping completed for Hukushima hotel 7.\n",
      "Scraping completed for Hukushima hotel 8.\n",
      "Scraping completed for Hukushima hotel 9.\n",
      "Scraping completed for Hukushima hotel 10.\n",
      "Scraping completed for Hukushima hotel 11.\n",
      "Scraping completed for Hukushima hotel 12.\n",
      "Scraping completed for Hukushima hotel 13.\n",
      "Scraping completed for Hukushima hotel 14.\n",
      "Scraping completed for Hukushima hotel 15.\n",
      "Scraping completed for Hukushima hotel 16.\n",
      "Scraping completed for Hukushima hotel 17.\n",
      "Scraping completed for Hukushima hotel 18.\n",
      "Scraping completed for Hukushima hotel 19.\n",
      "Scraping completed for Hukushima hotel 20.\n",
      "Scraping completed for Hukushima hotel 21.\n",
      "Scraping completed for Hukushima hotel 22.\n",
      "Scraping completed for Hukushima hotel 23.\n",
      "Scraping completed for Hukushima hotel 24.\n",
      "Scraping completed for Hukushima hotel 25.\n",
      "Scraping completed for Hukushima hotel 26.\n",
      "Scraping completed for Hukushima hotel 27.\n",
      "Scraping completed for Hukushima hotel 28.\n",
      "Scraping completed for Hukushima hotel 29.\n",
      "Scraping completed for Hukushima hotel 30.\n",
      "Scraping completed for Hukushima hotel 31.\n",
      "Scraping completed for Hukushima hotel 32.\n",
      "Scraping completed for Hukushima prefecture.\n",
      "All hotels scraped.\n"
     ]
    }
   ],
   "source": [
    "rakuten_dict = scrape_rakuten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we throw it into a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review_id</th>\n",
       "      <th>review_time</th>\n",
       "      <th>review_text</th>\n",
       "      <th>hotel_reply_time</th>\n",
       "      <th>hotel_reply_text</th>\n",
       "      <th>hotel_name</th>\n",
       "      <th>prefecture</th>\n",
       "      <th>overall_score</th>\n",
       "      <th>service_score</th>\n",
       "      <th>location_score</th>\n",
       "      <th>room_score</th>\n",
       "      <th>amenities_score</th>\n",
       "      <th>bathroom_score</th>\n",
       "      <th>food_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>voteans_21301527</td>\n",
       "      <td>2024-02-18 01:26:29</td>\n",
       "      <td>家族でのんびり過ごせて良かったです。お料理が美味しかったし、景色も美しかったです。温泉は思っ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>鶴の舞橋と岩木山　絶景の宿　つがる富士見荘</td>\n",
       "      <td>aomori</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>voteans_21251321</td>\n",
       "      <td>2024-01-29 12:03:50</td>\n",
       "      <td>ロビーからの鶴の舞橋の景観が素晴らしく、雪景色もあって楽しめました。食事も満腹となるほどの量...</td>\n",
       "      <td>2024-02-15 18:11:20</td>\n",
       "      <td>この度は当館にご宿泊いただき誠にありがとうございます。ロビーから見える鶴の舞橋と雪化粧した岩...</td>\n",
       "      <td>鶴の舞橋と岩木山　絶景の宿　つがる富士見荘</td>\n",
       "      <td>aomori</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>voteans_21170325</td>\n",
       "      <td>2023-12-26 15:04:31</td>\n",
       "      <td>先日はお世話になりました。夕食を部屋食への変更、ありがとうございました。お食事はとても味付け...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>鶴の舞橋と岩木山　絶景の宿　つがる富士見荘</td>\n",
       "      <td>aomori</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>voteans_21061209</td>\n",
       "      <td>2023-11-21 16:12:52</td>\n",
       "      <td>大変満足でした。お掃除も行き届いていましたしお料理も美味しかったです。お風呂のお湯も豊富で気...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>鶴の舞橋と岩木山　絶景の宿　つがる富士見荘</td>\n",
       "      <td>aomori</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>voteans_21031432</td>\n",
       "      <td>2023-11-12 19:16:54</td>\n",
       "      <td>【良かった点】・部屋から見える景色が素晴らしく大満足。・フロントの対応が丁寧。【気になった点...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>鶴の舞橋と岩木山　絶景の宿　つがる富士見荘</td>\n",
       "      <td>aomori</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          review_id          review_time  \\\n",
       "0  voteans_21301527  2024-02-18 01:26:29   \n",
       "1  voteans_21251321  2024-01-29 12:03:50   \n",
       "2  voteans_21170325  2023-12-26 15:04:31   \n",
       "3  voteans_21061209  2023-11-21 16:12:52   \n",
       "4  voteans_21031432  2023-11-12 19:16:54   \n",
       "\n",
       "                                         review_text     hotel_reply_time  \\\n",
       "0  家族でのんびり過ごせて良かったです。お料理が美味しかったし、景色も美しかったです。温泉は思っ...                  NaN   \n",
       "1  ロビーからの鶴の舞橋の景観が素晴らしく、雪景色もあって楽しめました。食事も満腹となるほどの量...  2024-02-15 18:11:20   \n",
       "2  先日はお世話になりました。夕食を部屋食への変更、ありがとうございました。お食事はとても味付け...                  NaN   \n",
       "3  大変満足でした。お掃除も行き届いていましたしお料理も美味しかったです。お風呂のお湯も豊富で気...                  NaN   \n",
       "4  【良かった点】・部屋から見える景色が素晴らしく大満足。・フロントの対応が丁寧。【気になった点...                  NaN   \n",
       "\n",
       "                                    hotel_reply_text             hotel_name  \\\n",
       "0                                                NaN  鶴の舞橋と岩木山　絶景の宿　つがる富士見荘   \n",
       "1  この度は当館にご宿泊いただき誠にありがとうございます。ロビーから見える鶴の舞橋と雪化粧した岩...  鶴の舞橋と岩木山　絶景の宿　つがる富士見荘   \n",
       "2                                                NaN  鶴の舞橋と岩木山　絶景の宿　つがる富士見荘   \n",
       "3                                                NaN  鶴の舞橋と岩木山　絶景の宿　つがる富士見荘   \n",
       "4                                                NaN  鶴の舞橋と岩木山　絶景の宿　つがる富士見荘   \n",
       "\n",
       "  prefecture overall_score service_score location_score room_score  \\\n",
       "0     aomori             4             4              4          4   \n",
       "1     aomori             4             4              5          4   \n",
       "2     aomori             4             5              5          5   \n",
       "3     aomori             5             4              5          4   \n",
       "4     aomori             2             2              5          2   \n",
       "\n",
       "  amenities_score bathroom_score food_score  \n",
       "0               4              4          5  \n",
       "1               4              4          4  \n",
       "2               4              5          5  \n",
       "3               3              4          4  \n",
       "4               3              2          2  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rakuten_df = pd.DataFrame.from_dict(rakuten_dict)\n",
    "rakuten_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we export it to a `.csv` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rakuten_df.to_csv('../datasets/rakuten_tohoku_reviews_replies.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
